{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of bert antonym.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqFBb56LQ0LX",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1jvh95xBAniCw_CENrfCFsAtZAIvAMDt4?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjrTVwMWLpAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "2778ea9a-bb41-475f-d041-c8e485d41ac5"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from scipy.spatial.distance import cosine\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 16.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.27)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.27 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.27)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.27->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.27->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 1131197.97B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE2GnW9hKKAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare(food):\n",
        "  # Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "  text = food[0]+\" \"+food[1]+\" \"+food[2]\n",
        "\n",
        "  # Add the special tokens.\n",
        "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "  # Split the sentence into tokens.\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "  # Map the token strings to their vocabulary indeces.\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  # Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "  segments_ids = [1] * len(tokenized_text)\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  model = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "  # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "  model.eval()\n",
        "  # Predict hidden states features for each layer\n",
        "  with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "  # Concatenate the tensors for all layers. We use `stack` here to\n",
        "  # create a new dimension in the tensor.\n",
        "  token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "  token_embeddings.size()\n",
        "  # Remove dimension 1, the \"batches\".\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "  token_embeddings.size()\n",
        "  # Swap dimensions 0 and 1.\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "  token_embeddings.size()\n",
        "  # Stores the token vectors, with shape [22 x 3,072]\n",
        "  token_vecs_cat = []\n",
        "\n",
        "  # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "  # For each token in the sentence...\n",
        "  for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "   \n",
        "  # Stores the token vectors, with shape [22 x 768]\n",
        "  token_vecs_sum = []\n",
        "\n",
        "  # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "  # For each token in the sentence...\n",
        "  for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        " \n",
        "  for i, token_str in enumerate(tokenized_text):\n",
        "    print (i, token_str)\n",
        "  # Calculate the cosine similarity between the word bank \n",
        "  # in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "  diff_bank = 1 - cosine(token_vecs_sum[2], token_vecs_sum[1])\n",
        "\n",
        "  # Calculate the cosine similarity between the word bank\n",
        "  # in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "  same_bank = 1 - cosine(token_vecs_sum[2], token_vecs_sum[3])\n",
        "\n",
        "  print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "  print('Vector similarity for *different* meanings:  %.2f' % diff_bank)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2bfDVd4Kyd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "46b126ac-bb5f-4907-b354-e55f18670a3e"
      },
      "source": [
        "fruits = [\"big\", \"large\", \"small\"]\n",
        "\n",
        "compare(fruits)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 big\n",
            "2 large\n",
            "3 small\n",
            "4 [SEP]\n",
            "Vector similarity for  *similar*  meanings:  0.92\n",
            "Vector similarity for *different* meanings:  0.89\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}